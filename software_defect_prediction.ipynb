{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\NITR_CS_PL4K\\Downloads\\JM1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79320b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ef579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f842d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf24b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea71229",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac5086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d09afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = df.shape\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fe716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad22832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LOC_BLA0K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac364c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode labels into numeric format\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)\n",
    "# y_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into features (X) and target (y)\n",
    "X = df.drop(columns=['Defective'])  # Replace 'target_column' with the name of your target column\n",
    "y = df['Defective']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set shape (features):\", X_train.shape)\n",
    "print(\"Testing set shape (features):\", X_test.shape)\n",
    "print(\"Training set shape (target):\", y_train.shape)\n",
    "print(\"Testing set shape (target):\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f84667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to NumPy array\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d70003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers with different kernels\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"SVM Sigmoid\": SVC(kernel='sigmoid', random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize dictionaries to store evaluation metrics\n",
    "metrics_dict = {\n",
    "    \"Classifier\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac44f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each classifier\n",
    "for clf_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_np, y_train)  # Use NumPy arrays instead of DataFrames\n",
    "    predictions = classifier.predict(X_test_np)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Create confusion matrix table\n",
    "    conf_matrix_table = PrettyTable()\n",
    "    conf_matrix_table.title = f\"Confusion Matrix for {clf_name}\"\n",
    "    conf_matrix_table.field_names = [\"\", \"Predicted 0\", \"Predicted 1\"]\n",
    "    \n",
    "    # Add rows for each actual class\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        conf_matrix_table.add_row([f\"Actual {i}\", conf_matrix[i][0], conf_matrix[i][1]])\n",
    "    \n",
    "    # Print confusion matrix for each classifier\n",
    "    print(conf_matrix_table)\n",
    "\n",
    "            \n",
    "    # Calculate metrics\n",
    "    accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "    precision = np.where(np.sum(conf_matrix, axis=0) == 0, 0, np.diag(conf_matrix) / np.sum(conf_matrix, axis=0))\n",
    "    recall = np.where(np.sum(conf_matrix, axis=1) == 0, 0, np.diag(conf_matrix) / np.sum(conf_matrix, axis=1))\n",
    "    f1_score = np.where((precision + recall) == 0, 0, 2 * precision * recall / (precision + recall))\n",
    "\n",
    "    # Store evaluation metrics in the dictionary\n",
    "    metrics_dict[\"Classifier\"].append(clf_name)\n",
    "    metrics_dict[\"Accuracy\"].append(accuracy)\n",
    "    metrics_dict[\"Precision\"].append(np.mean(precision))\n",
    "    metrics_dict[\"Recall\"].append(np.mean(recall))\n",
    "    metrics_dict[\"F1 Score\"].append(np.mean(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PrettyTable for evaluation metrics\n",
    "metrics_table = PrettyTable()\n",
    "metrics_table.field_names = [\"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "for i in range(len(metrics_dict[\"Classifier\"])):\n",
    "    metrics_table.add_row([\n",
    "        metrics_dict[\"Classifier\"][i],\n",
    "        metrics_dict[\"Accuracy\"][i],\n",
    "        metrics_dict[\"Precision\"][i],\n",
    "        metrics_dict[\"Recall\"][i],\n",
    "        metrics_dict[\"F1 Score\"][i]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation metrics table\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87990827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
